---
title: "abnormalInflation"
output: html_document
date: "2025-01-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lmerTest)
```


## Check for significance and abnormal inflation

To check for presence of behavioural signatures of causal selection found before by researchers Morris, Icard, etc. 

Abnormal *inflation* is where people disproportionately select the *rarer* cause in *conjunctive* settings.
Abnormal *deflation* is where people disproportionately select the more *common* cause in *disjunctive* settings.

Although our causal structures are complicated by the unobserved variables, we can check this phenomenon by comparing the observed variables A and B only, which are different in probability settings 1 and 3, at 0.1 and and 0.8 respectively.
(They are held equal in setting 2, while the unobserved variables vary)



```{r, include=FALSE}
# Get participant data
load('../Data/Data.Rdata', verbose = T) # This is one big df2, 2580 of 14

data <- df2 %>% 
  unite('pg_tt', pgroup, trialtype, sep = "_", remove = FALSE)

```

Filter this down to the trialtypes d7 and c5 (which are the disjunctive and conjunctive ones where 'everything happened', 111).
Tag this with whether or not they selected the lower-probability variable (which is A in each case). 


```{r, include=FALSE}
dfab <- data %>% # 92 obs of 15
  filter(pgroup %in% c(1, 3),
         trialtype %in% c('c5', 'd7'),
         node3 %in% c('A=1', 'B=1'))

# Variable P: allocate 1 when they choose the more normal variable (same in both pgroup 1 and 3)
dfab <- dfab %>% 
  mutate(P = case_when(
    node3 == 'B=1' ~ 1,
    node3 == 'A=1' ~ 0
  ))

dfab <- dfab %>% 
  select(subject_id, pg_tt, P) 

```


And for an lme4 significance test: exponentiate the Estimate to odds ratios. 
Only the coefficients/estimates on the link function scale—not standard errors, p-values, or z-values—should be exponentiated unless you're transforming confidence intervals.

```{r, include=FALSE}
#library(lme4)
#library(lmerTest)

predP <- glmer(P ~ 1 + (1|pg_tt) + (1|subject_id), data = dfab, family = binomial(link='logit')) # Also tried with + (1|subject_id) but no difference in fit, has convergence error and looks overparametrised?
summary(predP)

coef <- fixef(predP) # -0.445

est <- exp(coef) # 0.641 - exp converts logodds to odds. 
prob <- plogis(coef) # 0.39 - plogis is exp/(1+exp) and converts logodds to probs

se_intercept <- sqrt(diag(vcov(predP)))["(Intercept)"]

lower_logodds <- coef-(1.96*se_intercept)
upper_logodds <- coef+(1.96*se_intercept)

lower_or <- exp(lower_logodds)
upper_or <- exp(upper_logodds)

```

The conclusion here is that there is not strong enough difference in the two groups and so looks like no evidence for abnormal inflation and/or deflation.


## A new version

See plot 111abf in reportingFigs: instead of just comparing A and B, we compound whether the answer was associated with A or B. That puts the observed or unobserved together. We already made the plot for that, so now we need to analyse the data in the same way.

So we need a 'normalised probability pair'? Or can we just add eg. .5 and .1, .5 and .8

```{r}
dab <- data %>% 
  filter(trialtype %in% c('c5', 'd7'))
```


If you sum and normalise, then A and Au together is the low probability option in every pgroup:

```{r}
# First make the probs numeric not character
dab[, c("prob0", "prob1", "prob2", "prob3")] <- lapply(dab[, c("prob0", "prob1", "prob2", "prob3")], function(x) {
  as.numeric(sub("%", "", x)) / 100
})

dab$probA <- (dab$prob0 + dab$prob1) / (dab$prob0 + dab$prob1 + dab$prob2 + dab$prob3)
dab$probB <- (dab$prob2 + dab$prob3) / (dab$prob0 + dab$prob1 + dab$prob2 + dab$prob3)
```

Then give each a variable called P, like above:

```{r}

# Variable P: allocate 1 when they choose the more normal variable (same in both pgroup 1 and 3)
dab <- dab %>%
  mutate(P = case_when(
    startsWith(node3, "B") ~ 1,
    startsWith(node3, "A") ~ 0
  ))

dab <- dab %>% 
  select(subject_id, pg_tt, P) 

```


And run a glmer, which shows no difference! That means one conclusion from the cogsci25 paper (that participants well display abnormal selection) is no longer valid.

However we could do this in a more rigorous way in the model sampling script, like we do for unobserved variables (`sample_predictions.Rmd`).

```{r}
predPa <- glmer(P ~ 1 + (1|subject_id) + (1|pg_tt), data = dab, family = binomial(link='logit')) # Also tried with + (1|subject_id) but no difference in fit, has convergence error and looks overparametrised?
summary(predPa)

coef <- fixef(predPa) # -0.445

est <- exp(coef) # 0.641 - exp converts logodds to odds. 
prob <- plogis(coef) # 0.39 - plogis is exp/(1+exp) and converts logodds to probs

se_intercept <- sqrt(diag(vcov(predPa)))["(Intercept)"]

lower_logodds <- coef-(1.96*se_intercept)
upper_logodds <- coef+(1.96*se_intercept)

lower_or <- exp(lower_logodds)
upper_or <- exp(upper_logodds)

```