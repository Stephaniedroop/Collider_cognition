---
title: "sample_predictions"
output: html_document
date: "2025-09-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lmerTest)
```

## Generate samples from the model

The model predictions are currently probability distributions spread over the 8 node values (the 8 explanations for each possible outcome). We want to force the model to give an actual answer, to pair with each participant's observation. This cannot just be the same probability each time, but must have the chance to show up as a differnet node, proportional to its probability. It must be SAMPLED.

This needs the participant data by row, and then the model data too. 

There's more! NO need to modify the data in every script. Let's go back and do it in a oner.

```{r}
# Get model data
#df <- read.csv('../model_data/modelAndDataUnfitpn.csv') # 288 obs of 21
df <- read.csv('../model_data/fitforplot16mpn.csv') # 288 obs of 28

# Get participant data
load('../Data/Data.Rdata', verbose = T) # This is one big df2, 2580 of 14

datfortest <- df2 %>% select(subject_id, trial_id, node3, include)

justmp <- df %>% select(trial_id, node3, full)

# 39x9 - just the full model, in wide form
model_predictions_wide <- justmp %>% 
  pivot_wider(
    names_from = node3,
    values_from = full
  )

```


```{r}

```




```{r}
library(dplyr)

# Step 1: Extract variable names (probability columns) explicitly, excluding join key
variable_names <- setdiff(names(model_predictions_wide), "trial_id")

# Step 2: Define sampling function to pick one variable name based on probabilities
sample_variable_name <- function(probabilities, variable_names) {
  # probabilities: numeric vector for one row
  # variable_names: character vector of column names
  sampled_name <- sample(
    x = variable_names,
    size = 1,
    prob = as.numeric(probabilities)
  )
  return(as.character(sampled_name))
}

# Step 3: Join participant data with model predictions by condition ('pg_tt'),
# then for each participant (row), sample one variable name according to probabilities for their condition
sampled_predictions <- datfortest %>%
  left_join(model_predictions_wide, by = "trial_id") %>%
  rowwise() %>%
  mutate(
    # Select only the probability columns for sampling
    sampled_variable = sample_variable_name(
      probabilities = c_across(all_of(variable_names)),
      variable_names = variable_names
    )
  ) %>%
  ungroup() %>%
  select(subject_id, trial_id, sampled_variable)  # CHANGE THIS????

# Step 4: (optional) Inspect distribution of sampled variables by condition
prediction_distribution <- sampled_predictions %>%
  group_by(trial_id, sampled_variable) %>%
  summarize(count = n(), .groups = "drop")

print(prediction_distribution)

#sampled_predictions <- sampled_predictions[!duplicated(sampled_predictions[, c("subject_id", "trial_id", "sampled_variable")]), ]


```

Now merge back in to real participant data. This is for binomial logistic regression so needs certain vars like UNOBSERVED to be 1/0. Not sure what else we will use this for?

```{r}
modAndDat <- merge(df2, sampled_predictions, by = c("subject_id", "trial_id"), all.x = T)

modAndDat <- modAndDat[!duplicated(modAndDat[, c("subject_id", "trial_id", "sampled_variable")]), ] # For some reason 2556 not 2580?! Moving on for now...

# Assign variables
modAndDat <- modAndDat %>% mutate(Observed = factor(!node3%in%c('Au=0','Au=1','Bu=0', 'Bu=1'), levels = c(T,F)))

# No actually we need it long
modAndDat2 <- merge(datfortest, sampled_predictions) %>% rename(participants = node3, model = sampled_variable)

# Pivot longer
fortest <- pivot_longer(modAndDat2, cols = -c(subject_id, trial_id, include),
                        names_to = "respondent",
                                       values_to = "response")

fortest <- fortest %>% mutate(Observed = factor(!response%in%c('Au=0','Au=1','Bu=0', 'Bu=1'), levels = c(TRUE, FALSE)))
```

Now do the regression. It shows participants select OBSERVED less often than predicted by the model. This is reported in the section `Unobserved vs observed variables` section.

```{r}
predU <- glmer(Observed ~ respondent + (1|subject_id) + (1|trial_id), data = fortest, family = binomial(link='logit'))

summary(predU)

coef <- fixef(predU) # -0.202

est <- exp(coef) # 0.817 - exp converts logodds to odds. 
prob <- plogis(coef) # 0.39 - plogis is exp/(1+exp) and converts logodds to probs

se_intercept <- sqrt(diag(vcov(predU))) #["(Intercept)"]

lower_logodds <- coef-(1.96*se_intercept)
upper_logodds <- coef+(1.96*se_intercept)

lower_or <- exp(lower_logodds)
upper_or <- exp(upper_logodds)
```

TODO: what to do with this from now on. Should we also use it for the Actual/non-Actual analysis? It is more sophisticated.
TO DO: redo the plot for the unobserved section, using this sampled prediction data.