Knowledge==case$Knowledge,
Visible!=case$Visible) %>% select(visible=p_choose_hotdog)))
View(case)
dependence<-p_actual-p_counterfactual
#In this case we might reasonably blame her lazy character or the fact that the pizza was closer
Collapse
View(case)
View(pChoice)
dependence
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pref_match_hotdog <- as.numeric(pChoice$Preference=='Hotdog')
char_match_hotdog <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')
char_match_pizza <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog')
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog ie generative ie things that make you want hotdog
prom_hotdog <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog ie preventative factor ie things that amke you want pizza
prom_pizza <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(strengths[['character']], strengths[['preference']], baserate))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate)))
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1- strengths[['character']] *char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
22/110
2.002/10.10
5.005/10.01
2.005/10.1
2.002/10.1
52/110
exp(100)
exp(1)
runif(4)
sessionInfo()
sessionInfo()
sessionInfo()
install.packages("afex")
install.packages("faux")
library(broom, tidyverse, faux, afex)
library(broom)
library(faux)
library(afex)
R.version()
R.Version()
.libPaths()
.libPaths()
R.Version()
# ------- Prelims -----------
library(tidyverse)
library(ggplot2)
# ----------- Define an example prior df -------------------------
# Here define two causal vars and an exogenous noise variable for each (i.e. var epsilon A goes with A)
# in the exp setting this is 0.5
p_A <- c(.1,.9) # ie A usually has value 1... base rate for cause
p_epsA <- c(.7,.3) #... most of the time the noise var for a doesn't occur. for a to work it needs a and exp a. a is usually present but ogten doesnt work cos of noise term not working
p_B <- c(.8,.2) # B rarely fires 1...
p_epsB <- c(.3,.7) # but when it does it is strong
# And wrap them into a df called prior. Later the function should take dfs of this format:
# i.e. any number of causes as the rows, and the probs of them taking 0 and 1 as cols
params <- data.frame(rbind(p_A, p_epsA, p_B, p_epsB))
colnames(params) <- c(0,1)
# Other values set outside for now
N_cf <- 1000L # How many counterfactual samples to draw
s <- .7 # Stability
n_causes <- nrow(params)
causes <- rownames(params)
# Make a df of all combinations of variable settings
df <- expand.grid(rep(list(c(0,1)),n_causes), KEEP.OUT.ATTRS = F)
# ... with variables as the column names
colnames(df) <- causes
worlds <- nrow(df)
View(df)
structure <- 'disjunctive'
if (structure=="disjunctive") {
df$E <- as.numeric((df[1] & df[2]) | (df[3] & df[4]))
}
# Can replace with this - if rename - it is deterministic - literally gives specific outcome for set 3 causes, needs actual input. mechanical tell syou whether effects occurred given setting
# df$effect <- max( c(min(c1,e1), min(c2,e2), min(c3, e3), min(c2*c3, e23))) # BUT SAME PROBLEM - HOW TO AUTOMATICALLY DEAL WITH ANY NUMBER OF CAUSES?
mat <- as.matrix(df[,1:4])
View(mat)
# df2 <- as.matrix(df, dimnames=NULL)
# dimnames = list(c(1:16), c(causes))
# Replace every cell with the relevant indexed edge strength from params
for (k in 1:worlds){
for (cause in causes) {
a <- params[cause,df[k,cause]+1] # It needs the '+1' because r indexes from 1 not 0
mat[k,cause] <- a # ((then sometimes #*df[k,cause] if do at same time as structure but change later if need))
}
}
View(mat)
View(params)
# For each row of df, the prior is now the product of the same row of df2
df$Pr <- apply(mat, 1, prod) # parameter of the model
sum(df$Pr)
# Then loop to calculate cfs and assign causal responsibility
# Loop through possible world settings
for (c_ix in 1:worlds)
library(tidyverse)
df <- read.csv('modelAndDataUnfit2.csv') # 288 obs of 14
df <- read.csv('..model_data/modelAndDataUnfit2.csv') # 288 obs of 14
df <- read.csv('../model_data/modelAndDataUnfit2.csv') # 288 obs of 14
getwd()
setwd("~/Documents/GitHub/Collider_cognition/Main_scripts")
df <- read.csv('../model_data/modelAndDataUnfit2.csv') # 288 obs of 14
# Let's create variables coding the actual observation
df.map<-data.frame(condition=c('c1','c2','c3','c4','c5','d1','d2','d3','d4','d5','d6','d7'),
A=c(0,0,1,1,1, 0,0,0,1,1,1,1),
B=c(0,1,0,1,1, 0,1,1,0,0,1,1),
E=c(0,0,0,0,1, 0,0,1,0,1,0,1))
for (i in 1:nrow(df))
{
df$A[i]<-df.map$A[df.map$condition==df$trialtype[i]]
df$B[i]<-df.map$B[df.map$condition==df$trialtype[i]]
df$E[i]<-df.map$E[df.map$condition==df$trialtype[i]]
}
# df <- df %>%
#   mutate(include = !( (node3=='B=0' & B==1) | (node3=='B=1' & B==0) | (node3=='A=0' & A==1) | (node3=='A=1' & A==0)))
df <- df %>% rename(trial_id = pg_tt)
View(df)
#For testing:
pars <- c(1, 1, 1)
mod_name <- 'full'
i <- 1
model_likelihood <- function(pars, df, mod_name)
{
# If do them in order e,t then k, can repeat later by cutting out the k bits
epsilon <- exp(pars[1]) # 1/(1+ exp(-pars[2])) # TURN THIS TO
tau <- exp(pars[2])
kappa <- exp(pars[3])
tt <- unique(df$trial_id) #unique trial ids
nlls <- rep(NA, length(tt))#negative log likelihoods
for (i in 1:length(tt))
{
n <- df$n[df$trial_id==tt[i]] # & df$include==T] #get counts
mod_raw <- df[[mod_name]][df$trial_id==tt[i]] # & df$include==T] #get model predictions
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[df$trial_id==tt[i]] #& df$include==T]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kapp*dist)/tau) / sum(exp( (mod_raw + kapp*dist)/tau)))
nlls[i] <-  -sum(log(mod_pred2)*n) #get likelihood for this trial
}
sum(nlls)#return the total likelihood
}
#For testing:
pars <- c(1, 1, 1)
mod_name <- 'full'
i <- 1
model_likelihood <- function(pars, df, mod_name)
{
# If do them in order e,t then k, can repeat later by cutting out the k bits
epsilon <- exp(pars[1])
tau <- exp(pars[2])
kappa <- exp(pars[3])
tt <- unique(df$trial_id)
nlls <- rep(NA, length(tt))
for (i in 1:length(tt))
{
n <- df$n[df$trial_id==tt[i]]
mod_raw <- df[[mod_name]][df$trial_id==tt[i]]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[df$trial_id==tt[i]]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kapp*dist)/tau) / sum(exp( (mod_raw + kapp*dist)/tau)))
nlls[i] <-  -sum(log(mpred)*n) # Get likelihood for this trial
}
sum(nlls) # Return the total likelihood
}
generate_predictions <- function(mod_name, df, pars) {
tt <- unique(df$trial_id)
do.call(rbind, lapply(tt, function(t_id) {
trial_rows <- df$trial_id == t_id
mod_raw <- df[[mod_name]][trial_rows]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[trial_rows]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kapp*dist)/tau) / sum(exp( (mod_raw + kapp*dist)/tau)))
data.frame(
model = mod_name,
trial_id = t_id,
node3 = df$node3[trial_rows],
predicted_prob = mpred
)
}))
}
optimize_models <- function(model_names, df, initial_values = c(1, 1, 1)) {
optimize_single <- function(mod_name) {
result <- tryCatch({
optim(par = initial_values,
fn = model_likelihood,
df = df,
mod_name = mod_name)
}, error = function(e) {
message("Error in optimization for model ", mod_name, ": ", e$message)
return(list(par = c(NA, NA, NA), value = NA))
})
return(result)
}
out <- lapply(model_names, optimize_single)
names(out) <- model_names
mfs <- data.frame(
model = names(out),
eps = sapply(out, function(x) format(exp(x$par[1]), digits = 3, scientific = F)),
tau = exp(sapply(out, function(x) x$par[2])),
kap = exp(sapply(out, function(x) x$par[3])),
logl = -sapply(out, function(x) x$value)
) %>%
mutate(BIC = -2 * logl + 3 * log(sum(df$n)))
# To actually generate predictions
predictions <- do.call(rbind, lapply(names(out), function(mod_name) {
if(!any(is.na(out[[mod_name]]$par))) {
generate_predictions(
mod_name = mod_name,
df = df,
pars = exp(out[[mod_name]]$pars)
# Do I have to say the parameters separately, or can I just say pars?
#K = exp(out[[mod_name]]$par[1]),
#tau = exp(out[[mod_name]]$par[2]),
)
}
}))
list(
model_fits = mfs %>% mutate(K = format(K, digits=3), tau = format(tau, digits=3)),
predictions = predictions
)
}
# Usage:
model_names <- c('full',
'noAct',
'noInf',
'noSelect',
'noActnoInf',
'noActnoSelect',
'noInfnoSelect',
'noActnoInfnoSelect')
results <- optimize_models(model_names, df)
#For testing:
pars <- c(1, 1, 1)
mod_name <- 'full'
i <- 1
model_likelihood <- function(pars, df, mod_name)
{
# If do them in order e,t then k, can repeat later by cutting out the k bits
epsilon <- exp(pars[1])
tau <- exp(pars[2])
kappa <- exp(pars[3])
tt <- unique(df$trial_id)
nlls <- rep(NA, length(tt))
for (i in 1:length(tt))
{
n <- df$n[df$trial_id==tt[i]]
mod_raw <- df[[mod_name]][df$trial_id==tt[i]]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[df$trial_id==tt[i]]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kappa*dist)/tau) / sum(exp( (mod_raw + kappa*dist)/tau)))
nlls[i] <-  -sum(log(mpred)*n) # Get likelihood for this trial
}
sum(nlls) # Return the total likelihood
}
generate_predictions <- function(mod_name, df, pars) {
tt <- unique(df$trial_id)
do.call(rbind, lapply(tt, function(t_id) {
trial_rows <- df$trial_id == t_id
mod_raw <- df[[mod_name]][trial_rows]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[trial_rows]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kappa*dist)/tau) / sum(exp( (mod_raw + kappa*dist)/tau)))
data.frame(
model = mod_name,
trial_id = t_id,
node3 = df$node3[trial_rows],
predicted_prob = mpred
)
}))
}
optimize_models <- function(model_names, df, initial_values = c(1, 1, 1)) {
optimize_single <- function(mod_name) {
result <- tryCatch({
optim(par = initial_values,
fn = model_likelihood,
df = df,
mod_name = mod_name)
}, error = function(e) {
message("Error in optimization for model ", mod_name, ": ", e$message)
return(list(par = c(NA, NA, NA), value = NA))
})
return(result)
}
out <- lapply(model_names, optimize_single)
names(out) <- model_names
mfs <- data.frame(
model = names(out),
epsilon = sapply(out, function(x) format(exp(x$par[1]), digits = 3, scientific = F)),
tau = exp(sapply(out, function(x) x$par[2])),
kappa = exp(sapply(out, function(x) x$par[3])),
logl = -sapply(out, function(x) x$value)
) %>%
mutate(BIC = -2 * logl + 3 * log(sum(df$n)))
# To actually generate predictions
predictions <- do.call(rbind, lapply(names(out), function(mod_name) {
if(!any(is.na(out[[mod_name]]$par))) {
generate_predictions(
mod_name = mod_name,
df = df,
pars = exp(out[[mod_name]]$pars)
# Do I have to say the parameters separately, or can I just say pars?
#K = exp(out[[mod_name]]$par[1]),
#tau = exp(out[[mod_name]]$par[2]),
)
}
}))
list(
model_fits = mfs %>% mutate(epsilon = format(epsilon, digits=3), tau = format(tau, digits=3), kappa = format(kappa, digits=3)),
predictions = predictions
)
}
# Usage:
model_names <- c('full',
'noAct',
'noInf',
'noSelect',
'noActnoInf',
'noActnoSelect',
'noInfnoSelect',
'noActnoInfnoSelect')
results <- optimize_models(model_names, df)
print(results)
View(df)
df <- read.csv('../model_data/modelAndDataUnfit2.csv') # 288 obs of 14
# Let's create variables coding the actual observation
df.map<-data.frame(condition=c('c1','c2','c3','c4','c5','d1','d2','d3','d4','d5','d6','d7'),
A=c(0,0,1,1,1, 0,0,0,1,1,1,1),
B=c(0,1,0,1,1, 0,1,1,0,0,1,1),
E=c(0,0,0,0,1, 0,0,1,0,1,0,1))
for (i in 1:nrow(df))
{
df$A[i]<-df.map$A[df.map$condition==df$trialtype[i]]
df$B[i]<-df.map$B[df.map$condition==df$trialtype[i]]
df$E[i]<-df.map$E[df.map$condition==df$trialtype[i]]
}
# df <- df %>%
#   mutate(include = !( (node3=='B=0' & B==1) | (node3=='B=1' & B==0) | (node3=='A=0' & A==1) | (node3=='A=1' & A==0)))
df <- df %>% rename(trial_id = pg_tt)
df[is.na(df)] <- 0
#For testing:
pars <- c(1, 1, 1)
mod_name <- 'full'
i <- 1
model_likelihood <- function(pars, df, mod_name)
{
# If do them in order e,t then k, can repeat later by cutting out the k bits
epsilon <- exp(pars[1])
tau <- exp(pars[2])
kappa <- exp(pars[3])
tt <- unique(df$trial_id)
nlls <- rep(NA, length(tt))
for (i in 1:length(tt))
{
n <- df$n[df$trial_id==tt[i]]
mod_raw <- df[[mod_name]][df$trial_id==tt[i]]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[df$trial_id==tt[i]]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kappa*dist)/tau) / sum(exp( (mod_raw + kappa*dist)/tau)))
nlls[i] <-  -sum(log(mpred)*n) # Get likelihood for this trial
}
sum(nlls) # Return the total likelihood
}
generate_predictions <- function(mod_name, df, pars) {
tt <- unique(df$trial_id)
do.call(rbind, lapply(tt, function(t_id) {
trial_rows <- df$trial_id == t_id
mod_raw <- df[[mod_name]][trial_rows]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[trial_rows]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kappa*dist)/tau) / sum(exp( (mod_raw + kappa*dist)/tau)))
data.frame(
model = mod_name,
trial_id = t_id,
node3 = df$node3[trial_rows],
predicted_prob = mpred
)
}))
}
optimize_models <- function(model_names, df, initial_values = c(1, 1, 1)) {
optimize_single <- function(mod_name) {
result <- tryCatch({
optim(par = initial_values,
fn = model_likelihood,
df = df,
mod_name = mod_name)
}, error = function(e) {
message("Error in optimization for model ", mod_name, ": ", e$message)
return(list(par = c(NA, NA, NA), value = NA))
})
return(result)
}
out <- lapply(model_names, optimize_single)
names(out) <- model_names
mfs <- data.frame(
model = names(out),
epsilon = sapply(out, function(x) format(exp(x$par[1]), digits = 3, scientific = F)),
tau = exp(sapply(out, function(x) x$par[2])),
kappa = exp(sapply(out, function(x) x$par[3])),
logl = -sapply(out, function(x) x$value)
) %>%
mutate(BIC = -2 * logl + 3 * log(sum(df$n)))
# To actually generate predictions
predictions <- do.call(rbind, lapply(names(out), function(mod_name) {
if(!any(is.na(out[[mod_name]]$par))) {
generate_predictions(
mod_name = mod_name,
df = df,
pars = exp(out[[mod_name]]$pars)
# Do I have to say the parameters separately, or can I just say pars?
#K = exp(out[[mod_name]]$par[1]),
#tau = exp(out[[mod_name]]$par[2]),
)
}
}))
list(
model_fits = mfs %>% mutate(epsilon = format(epsilon, digits=3), tau = format(tau, digits=3), kappa = format(kappa, digits=3)),
predictions = predictions
)
}
# Usage:
model_names <- c('full',
'noAct',
'noInf',
'noSelect',
'noActnoInf',
'noActnoSelect',
'noInfnoSelect',
'noActnoInfnoSelect')
results <- optimize_models(model_names, df)
print(results)
warnings()
#For testing:
pars <- c(1, 1, 1)
mod_name <- 'full'
i <- 1
# If do them in order e,t then k, can repeat later by cutting out the k bits
epsilon <- exp(pars[1])
tau <- exp(pars[2])
kappa <- exp(pars[3])
tt <- unique(df$trial_id)
nlls <- rep(NA, length(tt))
n <- df$n[df$trial_id==tt[i]]
mod_raw <- df[[mod_name]][df$trial_id==tt[i]]
mod_raw[is.na(mod_raw)] <- 0
dist <- df$tv[df$trial_id==tt[i]]
dist[is.na(dist)] <- 0
mpred <- epsilon * (1/n) + (1-epsilon) * (exp( (mod_raw + kappa*dist)/tau) / sum(exp( (mod_raw + kappa*dist)/tau)))
